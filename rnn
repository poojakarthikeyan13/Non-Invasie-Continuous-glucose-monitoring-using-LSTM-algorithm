x = tf.placeholder('float', [None, n_chunks, chunk_size])
y = tf.placeholder('float')

# recurrent_neural_network() defines the RNN model. The network architecture
# used consists of a single LSTM cell followed by an output layer.
def recurrent_neural_network(x):
    layer = {'weights':tf.Variable(tf.random_normal([rnn_size, 1])),
             'biases':tf.Variable(tf.random_normal([1]))}

    # Reshape x to the format desired by the LSTM:
    x = tf.transpose(x, [1,0,2])
    x = tf.reshape(x, [-1, chunk_size])
    ##changed x as 0 and 0 as x
    x = tf.split(x, n_chunks, 0)

    ##replaced rnn_cell as rnn
    lstm_cell = rnn.BasicLSTMCell(rnn_size)##, state_is_tuple=True, activation=tf.nn.relu)
    ##replaced rnn.rnn as rnn.static_rnn and
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)

    output = tf.matmul(outputs[-1], layer['weights']) + layer['biases']

    return output

# The RNN is trained by feeding in sequences of glucose measurements seprarated
# by 10 minute intervals, and the desired output at a 20 minute prediction horizon.
